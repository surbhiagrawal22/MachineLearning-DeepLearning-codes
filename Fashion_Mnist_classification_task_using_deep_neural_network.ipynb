{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion Mnist classification task using deep neural network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM1GBUU6qshAR0msfjHu9vR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surbhiagrawal22/rep0/blob/master/Fashion_Mnist_classification_task_using_deep_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqWsVtujiM9S"
      },
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import fashion_mnist\n",
        "\n"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYkEuaSQkxVG"
      },
      "source": [
        "# reading the dataset\n",
        "(x_train, y_train), (x_test, y_test)=fashion_mnist.load_data()\n",
        "# checking shape of dataset\n",
        "print(f'The shape of X_train data {x_train.shape}')\n",
        "print(f'The shape of y_train data {y_train.shape}')\n",
        "print(f'The shape of X_test is {x_test.shape}')\n",
        "print(f'The shape of y_test is {y_test.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrzblZc6pSP9"
      },
      "source": [
        "# find out the number of unique labels\n",
        "unique_labels=np.unique(y_train)\n",
        "no_of_labels=len(unique_labels)\n",
        "\n",
        "print(f'The Unique labels are {unique_labels}')\n",
        "print(f'The number of labels are {no_of_labels}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0laDwm4kxjv"
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "# plotting train images\n",
        "for i in range(6):\n",
        "  plt.figure(figsize=[5,5])\n",
        "  plt.subplot(3,2,i+1)\n",
        "  # plt.subplot(3,2,i+1,figsize=[5,5]) # creating a subplot of 3 rows and 2 columns\n",
        "  plt.imshow(x_train[i])  \n",
        "  plt.title(f\"Actual Image Label {y_train[i]}\")\n",
        "  plt.subplots_adjust(hspace=2,wspace=1)\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYC4kwqIn1Ix"
      },
      "source": [
        "\n",
        "# plotting test images\n",
        "for i in range(6):\n",
        "  plt.subplot(3,2,i+1)\n",
        "  plt.imshow(x_test[i])\n",
        "  plt.title(f\"Actual Image Label {y_test[i]}\")\n",
        "  plt.subplots_adjust(hspace=.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQlx-fs9JZv2"
      },
      "source": [
        "DATA PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW5cjiB6kx0a"
      },
      "source": [
        "# Reshaping the data [batch, in_height, in_width, in_channels]\n",
        "x_train=x_train.reshape((-1,28,28,1))\n",
        "x_test=x_test.reshape((-1,28,28,1))\n",
        "\n",
        "# standarise the data\n",
        "x_train=x_train/255.0\n",
        "x_test=x_test/255.0"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iYGWzw7LGw8"
      },
      "source": [
        "# performing one-hot encoding - converting target data(y_train,y_test) into one-hot encoding \n",
        "from keras.utils import to_categorical\n",
        "y_train_hot_encoding=to_categorical(y_train)\n",
        "y_test_hot_encoding=to_categorical(y_test)\n",
        "print(y_train[:10])\n",
        "print(y_train_hot_encoding[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUbCA6-cNAK5"
      },
      "source": [
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_train_hot_encoding.shape)\n",
        "print(y_test_hot_encoding.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0B9op0wO2OO"
      },
      "source": [
        "Performing train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN2_sMVKO5q1"
      },
      "source": [
        "seed_no=7\n",
        "np.random.seed(seed_no)\n",
        "x_train_split,x_validation_split,y_train_split,y_validation_split=train_test_split(x_train,y_train_hot_encoding,test_size=.2,random_state=seed_no)\n",
        "print(x_train_split.shape)\n",
        "print(x_validation_split.shape)\n",
        "print(y_train_split.shape)\n",
        "print(y_validation_split.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_80UPLdJSykH"
      },
      "source": [
        "Defining a neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdQpP32VS3Yr"
      },
      "source": [
        "model=tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(32,kernel_size=(3,3),padding=\"same\",input_shape=(28,28,1),activation=\"linear\"))\n",
        "model.add(tf.keras.layers.LeakyReLU(alpha=0.1)) # default alpha value is =.3\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2),padding=\"same\"))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(64,kernel_size=(3,3),padding=\"same\",activation=\"linear\"))\n",
        "model.add(tf.keras.layers.LeakyReLU(alpha=0.1))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2),padding=\"same\"))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(128,kernel_size=(3,3),padding=\"same\"))\n",
        "model.add(tf.keras.layers.LeakyReLU(alpha=.1))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2),padding=\"same\")) # padding can have either two values, same or valid\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(128,activation=\"linear\"))\n",
        "model.add(tf.keras.layers.LeakyReLU(alpha=0.1))\n",
        "model.add(tf.keras.layers.Dense(no_of_labels,activation=\"softmax\"))\n"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGSjcgmBh7im"
      },
      "source": [
        "# comling a model\n",
        "model.compile(metrics=[\"accuracy\"],loss=\"categorical_crossentropy\",optimizer=\"adam\")\n",
        "# printing the mdoel summary\n",
        "print(model.summary())\n",
        "# print(model.get_weights())\n",
        "# print(model.get_layer(name=\"conv2d\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppw58NTLhnC1"
      },
      "source": [
        "Fitting a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XFhLQ-6fAlc"
      },
      "source": [
        "epochs=20\n",
        "batch_size=64\n",
        "fashion_model=model.fit(x_train_split,y_train_split,epochs=epochs,batch_size=batch_size,verbose=1,validation_data=(x_validation_split,y_validation_split))\n",
        "print(fashion_model.history)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}